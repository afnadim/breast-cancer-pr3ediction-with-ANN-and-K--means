{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#for the sigmoid function we need expit() from scipy\n",
    "import scipy.special\n",
    "#library for plotting arrays\n",
    "import matplotlib.pyplot as plt\n",
    "# A particularly interesting backend, provided by IPython, is the inline backend. \n",
    "# This is available only for the Jupyter Notebook and the Jupyter QtConsole. \n",
    "# It can be invoked as follows: %matplotlib inline\n",
    "# With this backend, the output of plotting commands is displayed inline \n",
    "# within frontends like the Jupyter notebook, directly below the code cell that produced it. \n",
    "# The resulting plots are inside this notebook, not an external window.\n",
    "\n",
    "import pandas as pd # to manage data frames and reading csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset and some pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"breast_cancer_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         M        17.99         10.38           122.8     1001.0   \n",
      "1    842517         M        20.57         17.77           132.9     1326.0   \n",
      "2  84300903         M        19.69         21.25           130.0     1203.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "\n",
      "      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
      "0     ...               17.33            184.6      2019.0            0.1622   \n",
      "1     ...               23.41            158.8      1956.0            0.1238   \n",
      "2     ...               25.53            152.5      1709.0            0.1444   \n",
      "\n",
      "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0             0.6656           0.7119                0.2654          0.4601   \n",
      "1             0.1866           0.2416                0.1860          0.2750   \n",
      "2             0.4245           0.4504                0.2430          0.3613   \n",
      "\n",
      "   fractal_dimension_worst  Unnamed: 32  \n",
      "0                  0.11890          NaN  \n",
      "1                  0.08902          NaN  \n",
      "2                  0.08758          NaN  \n",
      "\n",
      "[3 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnessary columns incluing id numbers\n",
    "df.drop(df.columns[[0,32]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0          M       17.990         10.38          122.80     1001.0   \n",
      "1          M       20.570         17.77          132.90     1326.0   \n",
      "2          M       19.690         21.25          130.00     1203.0   \n",
      "3          M       11.420         20.38           77.58      386.1   \n",
      "4          M       20.290         14.34          135.10     1297.0   \n",
      "5          M       12.450         15.70           82.57      477.1   \n",
      "6          M       18.250         19.98          119.60     1040.0   \n",
      "7          M       13.710         20.83           90.20      577.9   \n",
      "8          M       13.000         21.82           87.50      519.8   \n",
      "9          M       12.460         24.04           83.97      475.9   \n",
      "10         M       16.020         23.24          102.70      797.8   \n",
      "11         M       15.780         17.89          103.60      781.0   \n",
      "12         M       19.170         24.80          132.40     1123.0   \n",
      "13         M       15.850         23.95          103.70      782.7   \n",
      "14         M       13.730         22.61           93.60      578.3   \n",
      "15         M       14.540         27.54           96.73      658.8   \n",
      "16         M       14.680         20.13           94.74      684.5   \n",
      "17         M       16.130         20.68          108.10      798.8   \n",
      "18         M       19.810         22.15          130.00     1260.0   \n",
      "19         B       13.540         14.36           87.46      566.3   \n",
      "20         B       13.080         15.71           85.63      520.0   \n",
      "21         B        9.504         12.44           60.34      273.9   \n",
      "22         M       15.340         14.26          102.50      704.4   \n",
      "23         M       21.160         23.04          137.20     1404.0   \n",
      "24         M       16.650         21.38          110.00      904.6   \n",
      "25         M       17.140         16.40          116.00      912.7   \n",
      "26         M       14.580         21.53           97.41      644.8   \n",
      "27         M       18.610         20.25          122.10     1094.0   \n",
      "28         M       15.300         25.27          102.40      732.4   \n",
      "29         M       17.570         15.05          115.00      955.1   \n",
      "..       ...          ...           ...             ...        ...   \n",
      "70         M       18.940         21.31          123.60     1130.0   \n",
      "71         B        8.888         14.64           58.79      244.0   \n",
      "72         M       17.200         24.52          114.20      929.4   \n",
      "73         M       13.800         15.79           90.43      584.1   \n",
      "74         B       12.310         16.52           79.19      470.9   \n",
      "75         M       16.070         19.65          104.10      817.7   \n",
      "76         B       13.530         10.94           87.91      559.2   \n",
      "77         M       18.050         16.15          120.20     1006.0   \n",
      "78         M       20.180         23.97          143.70     1245.0   \n",
      "79         B       12.860         18.00           83.19      506.3   \n",
      "80         B       11.450         20.97           73.81      401.5   \n",
      "81         B       13.340         15.86           86.49      520.0   \n",
      "82         M       25.220         24.91          171.50     1878.0   \n",
      "83         M       19.100         26.29          129.10     1132.0   \n",
      "84         B       12.000         15.65           76.95      443.3   \n",
      "85         M       18.460         18.52          121.10     1075.0   \n",
      "86         M       14.480         21.46           94.25      648.2   \n",
      "87         M       19.020         24.59          122.00     1076.0   \n",
      "88         B       12.360         21.80           79.78      466.1   \n",
      "89         B       14.640         15.24           95.77      651.9   \n",
      "90         B       14.620         24.02           94.57      662.7   \n",
      "91         M       15.370         22.76          100.20      728.2   \n",
      "92         B       13.270         14.76           84.74      551.7   \n",
      "93         B       13.450         18.30           86.60      555.1   \n",
      "94         M       15.060         19.83          100.30      705.6   \n",
      "95         M       20.260         23.03          132.40     1264.0   \n",
      "96         B       12.180         17.84           77.79      451.1   \n",
      "97         B        9.787         19.94           62.11      294.5   \n",
      "98         B       11.600         12.84           74.34      412.6   \n",
      "99         M       14.420         19.77           94.48      642.5   \n",
      "\n",
      "    smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0           0.11840           0.27760        0.300100             0.147100   \n",
      "1           0.08474           0.07864        0.086900             0.070170   \n",
      "2           0.10960           0.15990        0.197400             0.127900   \n",
      "3           0.14250           0.28390        0.241400             0.105200   \n",
      "4           0.10030           0.13280        0.198000             0.104300   \n",
      "5           0.12780           0.17000        0.157800             0.080890   \n",
      "6           0.09463           0.10900        0.112700             0.074000   \n",
      "7           0.11890           0.16450        0.093660             0.059850   \n",
      "8           0.12730           0.19320        0.185900             0.093530   \n",
      "9           0.11860           0.23960        0.227300             0.085430   \n",
      "10          0.08206           0.06669        0.032990             0.033230   \n",
      "11          0.09710           0.12920        0.099540             0.066060   \n",
      "12          0.09740           0.24580        0.206500             0.111800   \n",
      "13          0.08401           0.10020        0.099380             0.053640   \n",
      "14          0.11310           0.22930        0.212800             0.080250   \n",
      "15          0.11390           0.15950        0.163900             0.073640   \n",
      "16          0.09867           0.07200        0.073950             0.052590   \n",
      "17          0.11700           0.20220        0.172200             0.102800   \n",
      "18          0.09831           0.10270        0.147900             0.094980   \n",
      "19          0.09779           0.08129        0.066640             0.047810   \n",
      "20          0.10750           0.12700        0.045680             0.031100   \n",
      "21          0.10240           0.06492        0.029560             0.020760   \n",
      "22          0.10730           0.21350        0.207700             0.097560   \n",
      "23          0.09428           0.10220        0.109700             0.086320   \n",
      "24          0.11210           0.14570        0.152500             0.091700   \n",
      "25          0.11860           0.22760        0.222900             0.140100   \n",
      "26          0.10540           0.18680        0.142500             0.087830   \n",
      "27          0.09440           0.10660        0.149000             0.077310   \n",
      "28          0.10820           0.16970        0.168300             0.087510   \n",
      "29          0.09847           0.11570        0.098750             0.079530   \n",
      "..              ...               ...             ...                  ...   \n",
      "70          0.09009           0.10290        0.108000             0.079510   \n",
      "71          0.09783           0.15310        0.086060             0.028720   \n",
      "72          0.10710           0.18300        0.169200             0.079440   \n",
      "73          0.10070           0.12800        0.077890             0.050690   \n",
      "74          0.09172           0.06829        0.033720             0.022720   \n",
      "75          0.09168           0.08424        0.097690             0.066380   \n",
      "76          0.12910           0.10470        0.068770             0.065560   \n",
      "77          0.10650           0.21460        0.168400             0.108000   \n",
      "78          0.12860           0.34540        0.375400             0.160400   \n",
      "79          0.09934           0.09546        0.038890             0.023150   \n",
      "80          0.11020           0.09362        0.045910             0.022330   \n",
      "81          0.10780           0.15350        0.116900             0.069870   \n",
      "82          0.10630           0.26650        0.333900             0.184500   \n",
      "83          0.12150           0.17910        0.193700             0.146900   \n",
      "84          0.09723           0.07165        0.041510             0.018630   \n",
      "85          0.09874           0.10530        0.133500             0.087950   \n",
      "86          0.09444           0.09947        0.120400             0.049380   \n",
      "87          0.09029           0.12060        0.146800             0.082710   \n",
      "88          0.08772           0.09445        0.060150             0.037450   \n",
      "89          0.11320           0.13390        0.099660             0.070640   \n",
      "90          0.08974           0.08606        0.031020             0.029570   \n",
      "91          0.09200           0.10360        0.112200             0.074830   \n",
      "92          0.07355           0.05055        0.032610             0.026480   \n",
      "93          0.10220           0.08165        0.039740             0.027800   \n",
      "94          0.10390           0.15530        0.170000             0.088150   \n",
      "95          0.09078           0.13130        0.146500             0.086830   \n",
      "96          0.10450           0.07057        0.024900             0.029410   \n",
      "97          0.10240           0.05301        0.006829             0.007937   \n",
      "98          0.08983           0.07525        0.041960             0.033500   \n",
      "99          0.09752           0.11410        0.093880             0.058390   \n",
      "\n",
      "    symmetry_mean           ...             radius_worst  texture_worst  \\\n",
      "0          0.2419           ...                   25.380          17.33   \n",
      "1          0.1812           ...                   24.990          23.41   \n",
      "2          0.2069           ...                   23.570          25.53   \n",
      "3          0.2597           ...                   14.910          26.50   \n",
      "4          0.1809           ...                   22.540          16.67   \n",
      "5          0.2087           ...                   15.470          23.75   \n",
      "6          0.1794           ...                   22.880          27.66   \n",
      "7          0.2196           ...                   17.060          28.14   \n",
      "8          0.2350           ...                   15.490          30.73   \n",
      "9          0.2030           ...                   15.090          40.68   \n",
      "10         0.1528           ...                   19.190          33.88   \n",
      "11         0.1842           ...                   20.420          27.28   \n",
      "12         0.2397           ...                   20.960          29.94   \n",
      "13         0.1847           ...                   16.840          27.66   \n",
      "14         0.2069           ...                   15.030          32.01   \n",
      "15         0.2303           ...                   17.460          37.13   \n",
      "16         0.1586           ...                   19.070          30.88   \n",
      "17         0.2164           ...                   20.960          31.48   \n",
      "18         0.1582           ...                   27.320          30.88   \n",
      "19         0.1885           ...                   15.110          19.26   \n",
      "20         0.1967           ...                   14.500          20.49   \n",
      "21         0.1815           ...                   10.230          15.66   \n",
      "22         0.2521           ...                   18.070          19.08   \n",
      "23         0.1769           ...                   29.170          35.59   \n",
      "24         0.1995           ...                   26.460          31.56   \n",
      "25         0.3040           ...                   22.250          21.40   \n",
      "26         0.2252           ...                   17.620          33.21   \n",
      "27         0.1697           ...                   21.310          27.26   \n",
      "28         0.1926           ...                   20.270          36.71   \n",
      "29         0.1739           ...                   20.010          19.52   \n",
      "..            ...           ...                      ...            ...   \n",
      "70         0.1582           ...                   24.860          26.58   \n",
      "71         0.1902           ...                    9.733          15.67   \n",
      "72         0.1927           ...                   23.320          33.82   \n",
      "73         0.1662           ...                   16.570          20.86   \n",
      "74         0.1720           ...                   14.110          23.21   \n",
      "75         0.1798           ...                   19.770          24.56   \n",
      "76         0.2403           ...                   14.080          12.49   \n",
      "77         0.2152           ...                   22.390          18.91   \n",
      "78         0.2906           ...                   23.370          31.72   \n",
      "79         0.1718           ...                   14.240          24.82   \n",
      "80         0.1842           ...                   13.110          32.16   \n",
      "81         0.1942           ...                   15.530          23.19   \n",
      "82         0.1829           ...                   30.000          33.62   \n",
      "83         0.1634           ...                   20.330          32.72   \n",
      "84         0.2079           ...                   13.670          24.90   \n",
      "85         0.2132           ...                   22.930          27.68   \n",
      "86         0.2075           ...                   16.210          29.25   \n",
      "87         0.1953           ...                   24.560          30.41   \n",
      "88         0.1930           ...                   13.830          30.50   \n",
      "89         0.2116           ...                   16.340          18.24   \n",
      "90         0.1685           ...                   16.110          29.11   \n",
      "91         0.1717           ...                   16.430          25.84   \n",
      "92         0.1386           ...                   16.360          22.35   \n",
      "93         0.1638           ...                   15.100          25.94   \n",
      "94         0.1855           ...                   18.230          24.23   \n",
      "95         0.2095           ...                   24.220          31.59   \n",
      "96         0.1900           ...                   12.830          20.92   \n",
      "97         0.1350           ...                   10.920          26.29   \n",
      "98         0.1620           ...                   13.060          17.16   \n",
      "99         0.1879           ...                   16.330          30.86   \n",
      "\n",
      "    perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
      "0            184.60      2019.0            0.1622            0.66560   \n",
      "1            158.80      1956.0            0.1238            0.18660   \n",
      "2            152.50      1709.0            0.1444            0.42450   \n",
      "3             98.87       567.7            0.2098            0.86630   \n",
      "4            152.20      1575.0            0.1374            0.20500   \n",
      "5            103.40       741.6            0.1791            0.52490   \n",
      "6            153.20      1606.0            0.1442            0.25760   \n",
      "7            110.60       897.0            0.1654            0.36820   \n",
      "8            106.20       739.3            0.1703            0.54010   \n",
      "9             97.65       711.4            0.1853            1.05800   \n",
      "10           123.80      1150.0            0.1181            0.15510   \n",
      "11           136.50      1299.0            0.1396            0.56090   \n",
      "12           151.70      1332.0            0.1037            0.39030   \n",
      "13           112.00       876.5            0.1131            0.19240   \n",
      "14           108.80       697.7            0.1651            0.77250   \n",
      "15           124.10       943.2            0.1678            0.65770   \n",
      "16           123.40      1138.0            0.1464            0.18710   \n",
      "17           136.80      1315.0            0.1789            0.42330   \n",
      "18           186.80      2398.0            0.1512            0.31500   \n",
      "19            99.70       711.2            0.1440            0.17730   \n",
      "20            96.09       630.5            0.1312            0.27760   \n",
      "21            65.13       314.9            0.1324            0.11480   \n",
      "22           125.10       980.9            0.1390            0.59540   \n",
      "23           188.00      2615.0            0.1401            0.26000   \n",
      "24           177.00      2215.0            0.1805            0.35780   \n",
      "25           152.40      1461.0            0.1545            0.39490   \n",
      "26           122.40       896.9            0.1525            0.66430   \n",
      "27           139.90      1403.0            0.1338            0.21170   \n",
      "28           149.30      1269.0            0.1641            0.61100   \n",
      "29           134.90      1227.0            0.1255            0.28120   \n",
      "..              ...         ...               ...                ...   \n",
      "70           165.90      1866.0            0.1193            0.23360   \n",
      "71            62.56       284.4            0.1207            0.24360   \n",
      "72           151.60      1681.0            0.1585            0.73940   \n",
      "73           110.30       812.4            0.1411            0.35420   \n",
      "74            89.71       611.1            0.1176            0.18430   \n",
      "75           128.80      1223.0            0.1500            0.20450   \n",
      "76            91.36       605.5            0.1451            0.13790   \n",
      "77           150.10      1610.0            0.1478            0.56340   \n",
      "78           170.30      1623.0            0.1639            0.61640   \n",
      "79            91.88       622.1            0.1289            0.21410   \n",
      "80            84.53       525.1            0.1557            0.16760   \n",
      "81            96.66       614.9            0.1536            0.47910   \n",
      "82           211.70      2562.0            0.1573            0.60760   \n",
      "83           141.30      1298.0            0.1392            0.28170   \n",
      "84            87.78       567.9            0.1377            0.20030   \n",
      "85           152.20      1603.0            0.1398            0.20890   \n",
      "86           108.40       808.9            0.1306            0.19760   \n",
      "87           152.90      1623.0            0.1249            0.32060   \n",
      "88            91.46       574.7            0.1304            0.24630   \n",
      "89           109.40       803.6            0.1277            0.30890   \n",
      "90           102.90       803.7            0.1115            0.17660   \n",
      "91           107.50       830.9            0.1257            0.19970   \n",
      "92           104.50       830.6            0.1006            0.12380   \n",
      "93            97.59       699.4            0.1339            0.17510   \n",
      "94           123.50      1025.0            0.1551            0.42030   \n",
      "95           156.10      1750.0            0.1190            0.35390   \n",
      "96            82.14       495.2            0.1140            0.09358   \n",
      "97            68.81       366.1            0.1316            0.09473   \n",
      "98            82.96       512.5            0.1431            0.18510   \n",
      "99           109.50       826.4            0.1431            0.30260   \n",
      "\n",
      "    concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0           0.71190               0.26540          0.4601   \n",
      "1           0.24160               0.18600          0.2750   \n",
      "2           0.45040               0.24300          0.3613   \n",
      "3           0.68690               0.25750          0.6638   \n",
      "4           0.40000               0.16250          0.2364   \n",
      "5           0.53550               0.17410          0.3985   \n",
      "6           0.37840               0.19320          0.3063   \n",
      "7           0.26780               0.15560          0.3196   \n",
      "8           0.53900               0.20600          0.4378   \n",
      "9           1.10500               0.22100          0.4366   \n",
      "10          0.14590               0.09975          0.2948   \n",
      "11          0.39650               0.18100          0.3792   \n",
      "12          0.36390               0.17670          0.3176   \n",
      "13          0.23220               0.11190          0.2809   \n",
      "14          0.69430               0.22080          0.3596   \n",
      "15          0.70260               0.17120          0.4218   \n",
      "16          0.29140               0.16090          0.3029   \n",
      "17          0.47840               0.20730          0.3706   \n",
      "18          0.53720               0.23880          0.2768   \n",
      "19          0.23900               0.12880          0.2977   \n",
      "20          0.18900               0.07283          0.3184   \n",
      "21          0.08867               0.06227          0.2450   \n",
      "22          0.63050               0.23930          0.4667   \n",
      "23          0.31550               0.20090          0.2822   \n",
      "24          0.46950               0.20950          0.3613   \n",
      "25          0.38530               0.25500          0.4066   \n",
      "26          0.55390               0.27010          0.4264   \n",
      "27          0.34460               0.14900          0.2341   \n",
      "28          0.63350               0.20240          0.4027   \n",
      "29          0.24890               0.14560          0.2756   \n",
      "..              ...                   ...             ...   \n",
      "70          0.26870               0.17890          0.2551   \n",
      "71          0.14340               0.04786          0.2254   \n",
      "72          0.65660               0.18990          0.3313   \n",
      "73          0.27790               0.13830          0.2589   \n",
      "74          0.17030               0.08660          0.2618   \n",
      "75          0.28290               0.15200          0.2650   \n",
      "76          0.08539               0.07407          0.2710   \n",
      "77          0.37860               0.21020          0.3751   \n",
      "78          0.76810               0.25080          0.5440   \n",
      "79          0.17310               0.07926          0.2779   \n",
      "80          0.17550               0.06127          0.2762   \n",
      "81          0.48580               0.17080          0.3527   \n",
      "82          0.64760               0.28670          0.2355   \n",
      "83          0.24320               0.18410          0.2311   \n",
      "84          0.22670               0.07632          0.3379   \n",
      "85          0.31570               0.16420          0.3695   \n",
      "86          0.33490               0.12250          0.3020   \n",
      "87          0.57550               0.19560          0.3956   \n",
      "88          0.24340               0.12050          0.2972   \n",
      "89          0.26040               0.13970          0.3151   \n",
      "90          0.09189               0.06946          0.2522   \n",
      "91          0.28460               0.14760          0.2556   \n",
      "92          0.13500               0.10010          0.2027   \n",
      "93          0.13810               0.07911          0.2678   \n",
      "94          0.52030               0.21150          0.2834   \n",
      "95          0.40980               0.15730          0.3689   \n",
      "96          0.04980               0.05882          0.2227   \n",
      "97          0.02049               0.02381          0.1934   \n",
      "98          0.19220               0.08449          0.2772   \n",
      "99          0.31940               0.15650          0.2718   \n",
      "\n",
      "    fractal_dimension_worst  \n",
      "0                   0.11890  \n",
      "1                   0.08902  \n",
      "2                   0.08758  \n",
      "3                   0.17300  \n",
      "4                   0.07678  \n",
      "5                   0.12440  \n",
      "6                   0.08368  \n",
      "7                   0.11510  \n",
      "8                   0.10720  \n",
      "9                   0.20750  \n",
      "10                  0.08452  \n",
      "11                  0.10480  \n",
      "12                  0.10230  \n",
      "13                  0.06287  \n",
      "14                  0.14310  \n",
      "15                  0.13410  \n",
      "16                  0.08216  \n",
      "17                  0.11420  \n",
      "18                  0.07615  \n",
      "19                  0.07259  \n",
      "20                  0.08183  \n",
      "21                  0.07773  \n",
      "22                  0.09946  \n",
      "23                  0.07526  \n",
      "24                  0.09564  \n",
      "25                  0.10590  \n",
      "26                  0.12750  \n",
      "27                  0.07421  \n",
      "28                  0.09876  \n",
      "29                  0.07919  \n",
      "..                      ...  \n",
      "70                  0.06589  \n",
      "71                  0.10840  \n",
      "72                  0.13390  \n",
      "73                  0.10300  \n",
      "74                  0.07609  \n",
      "75                  0.06387  \n",
      "76                  0.07191  \n",
      "77                  0.11080  \n",
      "78                  0.09964  \n",
      "79                  0.07918  \n",
      "80                  0.08851  \n",
      "81                  0.10160  \n",
      "82                  0.10510  \n",
      "83                  0.09203  \n",
      "84                  0.07924  \n",
      "85                  0.08579  \n",
      "86                  0.06846  \n",
      "87                  0.09288  \n",
      "88                  0.09261  \n",
      "89                  0.08473  \n",
      "90                  0.07246  \n",
      "91                  0.06828  \n",
      "92                  0.06206  \n",
      "93                  0.06603  \n",
      "94                  0.08234  \n",
      "95                  0.08368  \n",
      "96                  0.07376  \n",
      "97                  0.08988  \n",
      "98                  0.08756  \n",
      "99                  0.09353  \n",
      "\n",
      "[100 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binariseing the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0           0       17.990         10.38          122.80     1001.0   \n",
      "1           0       20.570         17.77          132.90     1326.0   \n",
      "2           0       19.690         21.25          130.00     1203.0   \n",
      "3           0       11.420         20.38           77.58      386.1   \n",
      "4           0       20.290         14.34          135.10     1297.0   \n",
      "5           0       12.450         15.70           82.57      477.1   \n",
      "6           0       18.250         19.98          119.60     1040.0   \n",
      "7           0       13.710         20.83           90.20      577.9   \n",
      "8           0       13.000         21.82           87.50      519.8   \n",
      "9           0       12.460         24.04           83.97      475.9   \n",
      "10          0       16.020         23.24          102.70      797.8   \n",
      "11          0       15.780         17.89          103.60      781.0   \n",
      "12          0       19.170         24.80          132.40     1123.0   \n",
      "13          0       15.850         23.95          103.70      782.7   \n",
      "14          0       13.730         22.61           93.60      578.3   \n",
      "15          0       14.540         27.54           96.73      658.8   \n",
      "16          0       14.680         20.13           94.74      684.5   \n",
      "17          0       16.130         20.68          108.10      798.8   \n",
      "18          0       19.810         22.15          130.00     1260.0   \n",
      "19          1       13.540         14.36           87.46      566.3   \n",
      "20          1       13.080         15.71           85.63      520.0   \n",
      "21          1        9.504         12.44           60.34      273.9   \n",
      "22          0       15.340         14.26          102.50      704.4   \n",
      "23          0       21.160         23.04          137.20     1404.0   \n",
      "24          0       16.650         21.38          110.00      904.6   \n",
      "25          0       17.140         16.40          116.00      912.7   \n",
      "26          0       14.580         21.53           97.41      644.8   \n",
      "27          0       18.610         20.25          122.10     1094.0   \n",
      "28          0       15.300         25.27          102.40      732.4   \n",
      "29          0       17.570         15.05          115.00      955.1   \n",
      "..        ...          ...           ...             ...        ...   \n",
      "70          0       18.940         21.31          123.60     1130.0   \n",
      "71          1        8.888         14.64           58.79      244.0   \n",
      "72          0       17.200         24.52          114.20      929.4   \n",
      "73          0       13.800         15.79           90.43      584.1   \n",
      "74          1       12.310         16.52           79.19      470.9   \n",
      "75          0       16.070         19.65          104.10      817.7   \n",
      "76          1       13.530         10.94           87.91      559.2   \n",
      "77          0       18.050         16.15          120.20     1006.0   \n",
      "78          0       20.180         23.97          143.70     1245.0   \n",
      "79          1       12.860         18.00           83.19      506.3   \n",
      "80          1       11.450         20.97           73.81      401.5   \n",
      "81          1       13.340         15.86           86.49      520.0   \n",
      "82          0       25.220         24.91          171.50     1878.0   \n",
      "83          0       19.100         26.29          129.10     1132.0   \n",
      "84          1       12.000         15.65           76.95      443.3   \n",
      "85          0       18.460         18.52          121.10     1075.0   \n",
      "86          0       14.480         21.46           94.25      648.2   \n",
      "87          0       19.020         24.59          122.00     1076.0   \n",
      "88          1       12.360         21.80           79.78      466.1   \n",
      "89          1       14.640         15.24           95.77      651.9   \n",
      "90          1       14.620         24.02           94.57      662.7   \n",
      "91          0       15.370         22.76          100.20      728.2   \n",
      "92          1       13.270         14.76           84.74      551.7   \n",
      "93          1       13.450         18.30           86.60      555.1   \n",
      "94          0       15.060         19.83          100.30      705.6   \n",
      "95          0       20.260         23.03          132.40     1264.0   \n",
      "96          1       12.180         17.84           77.79      451.1   \n",
      "97          1        9.787         19.94           62.11      294.5   \n",
      "98          1       11.600         12.84           74.34      412.6   \n",
      "99          0       14.420         19.77           94.48      642.5   \n",
      "\n",
      "    smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0           0.11840           0.27760        0.300100             0.147100   \n",
      "1           0.08474           0.07864        0.086900             0.070170   \n",
      "2           0.10960           0.15990        0.197400             0.127900   \n",
      "3           0.14250           0.28390        0.241400             0.105200   \n",
      "4           0.10030           0.13280        0.198000             0.104300   \n",
      "5           0.12780           0.17000        0.157800             0.080890   \n",
      "6           0.09463           0.10900        0.112700             0.074000   \n",
      "7           0.11890           0.16450        0.093660             0.059850   \n",
      "8           0.12730           0.19320        0.185900             0.093530   \n",
      "9           0.11860           0.23960        0.227300             0.085430   \n",
      "10          0.08206           0.06669        0.032990             0.033230   \n",
      "11          0.09710           0.12920        0.099540             0.066060   \n",
      "12          0.09740           0.24580        0.206500             0.111800   \n",
      "13          0.08401           0.10020        0.099380             0.053640   \n",
      "14          0.11310           0.22930        0.212800             0.080250   \n",
      "15          0.11390           0.15950        0.163900             0.073640   \n",
      "16          0.09867           0.07200        0.073950             0.052590   \n",
      "17          0.11700           0.20220        0.172200             0.102800   \n",
      "18          0.09831           0.10270        0.147900             0.094980   \n",
      "19          0.09779           0.08129        0.066640             0.047810   \n",
      "20          0.10750           0.12700        0.045680             0.031100   \n",
      "21          0.10240           0.06492        0.029560             0.020760   \n",
      "22          0.10730           0.21350        0.207700             0.097560   \n",
      "23          0.09428           0.10220        0.109700             0.086320   \n",
      "24          0.11210           0.14570        0.152500             0.091700   \n",
      "25          0.11860           0.22760        0.222900             0.140100   \n",
      "26          0.10540           0.18680        0.142500             0.087830   \n",
      "27          0.09440           0.10660        0.149000             0.077310   \n",
      "28          0.10820           0.16970        0.168300             0.087510   \n",
      "29          0.09847           0.11570        0.098750             0.079530   \n",
      "..              ...               ...             ...                  ...   \n",
      "70          0.09009           0.10290        0.108000             0.079510   \n",
      "71          0.09783           0.15310        0.086060             0.028720   \n",
      "72          0.10710           0.18300        0.169200             0.079440   \n",
      "73          0.10070           0.12800        0.077890             0.050690   \n",
      "74          0.09172           0.06829        0.033720             0.022720   \n",
      "75          0.09168           0.08424        0.097690             0.066380   \n",
      "76          0.12910           0.10470        0.068770             0.065560   \n",
      "77          0.10650           0.21460        0.168400             0.108000   \n",
      "78          0.12860           0.34540        0.375400             0.160400   \n",
      "79          0.09934           0.09546        0.038890             0.023150   \n",
      "80          0.11020           0.09362        0.045910             0.022330   \n",
      "81          0.10780           0.15350        0.116900             0.069870   \n",
      "82          0.10630           0.26650        0.333900             0.184500   \n",
      "83          0.12150           0.17910        0.193700             0.146900   \n",
      "84          0.09723           0.07165        0.041510             0.018630   \n",
      "85          0.09874           0.10530        0.133500             0.087950   \n",
      "86          0.09444           0.09947        0.120400             0.049380   \n",
      "87          0.09029           0.12060        0.146800             0.082710   \n",
      "88          0.08772           0.09445        0.060150             0.037450   \n",
      "89          0.11320           0.13390        0.099660             0.070640   \n",
      "90          0.08974           0.08606        0.031020             0.029570   \n",
      "91          0.09200           0.10360        0.112200             0.074830   \n",
      "92          0.07355           0.05055        0.032610             0.026480   \n",
      "93          0.10220           0.08165        0.039740             0.027800   \n",
      "94          0.10390           0.15530        0.170000             0.088150   \n",
      "95          0.09078           0.13130        0.146500             0.086830   \n",
      "96          0.10450           0.07057        0.024900             0.029410   \n",
      "97          0.10240           0.05301        0.006829             0.007937   \n",
      "98          0.08983           0.07525        0.041960             0.033500   \n",
      "99          0.09752           0.11410        0.093880             0.058390   \n",
      "\n",
      "    symmetry_mean           ...             radius_worst  texture_worst  \\\n",
      "0          0.2419           ...                   25.380          17.33   \n",
      "1          0.1812           ...                   24.990          23.41   \n",
      "2          0.2069           ...                   23.570          25.53   \n",
      "3          0.2597           ...                   14.910          26.50   \n",
      "4          0.1809           ...                   22.540          16.67   \n",
      "5          0.2087           ...                   15.470          23.75   \n",
      "6          0.1794           ...                   22.880          27.66   \n",
      "7          0.2196           ...                   17.060          28.14   \n",
      "8          0.2350           ...                   15.490          30.73   \n",
      "9          0.2030           ...                   15.090          40.68   \n",
      "10         0.1528           ...                   19.190          33.88   \n",
      "11         0.1842           ...                   20.420          27.28   \n",
      "12         0.2397           ...                   20.960          29.94   \n",
      "13         0.1847           ...                   16.840          27.66   \n",
      "14         0.2069           ...                   15.030          32.01   \n",
      "15         0.2303           ...                   17.460          37.13   \n",
      "16         0.1586           ...                   19.070          30.88   \n",
      "17         0.2164           ...                   20.960          31.48   \n",
      "18         0.1582           ...                   27.320          30.88   \n",
      "19         0.1885           ...                   15.110          19.26   \n",
      "20         0.1967           ...                   14.500          20.49   \n",
      "21         0.1815           ...                   10.230          15.66   \n",
      "22         0.2521           ...                   18.070          19.08   \n",
      "23         0.1769           ...                   29.170          35.59   \n",
      "24         0.1995           ...                   26.460          31.56   \n",
      "25         0.3040           ...                   22.250          21.40   \n",
      "26         0.2252           ...                   17.620          33.21   \n",
      "27         0.1697           ...                   21.310          27.26   \n",
      "28         0.1926           ...                   20.270          36.71   \n",
      "29         0.1739           ...                   20.010          19.52   \n",
      "..            ...           ...                      ...            ...   \n",
      "70         0.1582           ...                   24.860          26.58   \n",
      "71         0.1902           ...                    9.733          15.67   \n",
      "72         0.1927           ...                   23.320          33.82   \n",
      "73         0.1662           ...                   16.570          20.86   \n",
      "74         0.1720           ...                   14.110          23.21   \n",
      "75         0.1798           ...                   19.770          24.56   \n",
      "76         0.2403           ...                   14.080          12.49   \n",
      "77         0.2152           ...                   22.390          18.91   \n",
      "78         0.2906           ...                   23.370          31.72   \n",
      "79         0.1718           ...                   14.240          24.82   \n",
      "80         0.1842           ...                   13.110          32.16   \n",
      "81         0.1942           ...                   15.530          23.19   \n",
      "82         0.1829           ...                   30.000          33.62   \n",
      "83         0.1634           ...                   20.330          32.72   \n",
      "84         0.2079           ...                   13.670          24.90   \n",
      "85         0.2132           ...                   22.930          27.68   \n",
      "86         0.2075           ...                   16.210          29.25   \n",
      "87         0.1953           ...                   24.560          30.41   \n",
      "88         0.1930           ...                   13.830          30.50   \n",
      "89         0.2116           ...                   16.340          18.24   \n",
      "90         0.1685           ...                   16.110          29.11   \n",
      "91         0.1717           ...                   16.430          25.84   \n",
      "92         0.1386           ...                   16.360          22.35   \n",
      "93         0.1638           ...                   15.100          25.94   \n",
      "94         0.1855           ...                   18.230          24.23   \n",
      "95         0.2095           ...                   24.220          31.59   \n",
      "96         0.1900           ...                   12.830          20.92   \n",
      "97         0.1350           ...                   10.920          26.29   \n",
      "98         0.1620           ...                   13.060          17.16   \n",
      "99         0.1879           ...                   16.330          30.86   \n",
      "\n",
      "    perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
      "0            184.60      2019.0            0.1622            0.66560   \n",
      "1            158.80      1956.0            0.1238            0.18660   \n",
      "2            152.50      1709.0            0.1444            0.42450   \n",
      "3             98.87       567.7            0.2098            0.86630   \n",
      "4            152.20      1575.0            0.1374            0.20500   \n",
      "5            103.40       741.6            0.1791            0.52490   \n",
      "6            153.20      1606.0            0.1442            0.25760   \n",
      "7            110.60       897.0            0.1654            0.36820   \n",
      "8            106.20       739.3            0.1703            0.54010   \n",
      "9             97.65       711.4            0.1853            1.05800   \n",
      "10           123.80      1150.0            0.1181            0.15510   \n",
      "11           136.50      1299.0            0.1396            0.56090   \n",
      "12           151.70      1332.0            0.1037            0.39030   \n",
      "13           112.00       876.5            0.1131            0.19240   \n",
      "14           108.80       697.7            0.1651            0.77250   \n",
      "15           124.10       943.2            0.1678            0.65770   \n",
      "16           123.40      1138.0            0.1464            0.18710   \n",
      "17           136.80      1315.0            0.1789            0.42330   \n",
      "18           186.80      2398.0            0.1512            0.31500   \n",
      "19            99.70       711.2            0.1440            0.17730   \n",
      "20            96.09       630.5            0.1312            0.27760   \n",
      "21            65.13       314.9            0.1324            0.11480   \n",
      "22           125.10       980.9            0.1390            0.59540   \n",
      "23           188.00      2615.0            0.1401            0.26000   \n",
      "24           177.00      2215.0            0.1805            0.35780   \n",
      "25           152.40      1461.0            0.1545            0.39490   \n",
      "26           122.40       896.9            0.1525            0.66430   \n",
      "27           139.90      1403.0            0.1338            0.21170   \n",
      "28           149.30      1269.0            0.1641            0.61100   \n",
      "29           134.90      1227.0            0.1255            0.28120   \n",
      "..              ...         ...               ...                ...   \n",
      "70           165.90      1866.0            0.1193            0.23360   \n",
      "71            62.56       284.4            0.1207            0.24360   \n",
      "72           151.60      1681.0            0.1585            0.73940   \n",
      "73           110.30       812.4            0.1411            0.35420   \n",
      "74            89.71       611.1            0.1176            0.18430   \n",
      "75           128.80      1223.0            0.1500            0.20450   \n",
      "76            91.36       605.5            0.1451            0.13790   \n",
      "77           150.10      1610.0            0.1478            0.56340   \n",
      "78           170.30      1623.0            0.1639            0.61640   \n",
      "79            91.88       622.1            0.1289            0.21410   \n",
      "80            84.53       525.1            0.1557            0.16760   \n",
      "81            96.66       614.9            0.1536            0.47910   \n",
      "82           211.70      2562.0            0.1573            0.60760   \n",
      "83           141.30      1298.0            0.1392            0.28170   \n",
      "84            87.78       567.9            0.1377            0.20030   \n",
      "85           152.20      1603.0            0.1398            0.20890   \n",
      "86           108.40       808.9            0.1306            0.19760   \n",
      "87           152.90      1623.0            0.1249            0.32060   \n",
      "88            91.46       574.7            0.1304            0.24630   \n",
      "89           109.40       803.6            0.1277            0.30890   \n",
      "90           102.90       803.7            0.1115            0.17660   \n",
      "91           107.50       830.9            0.1257            0.19970   \n",
      "92           104.50       830.6            0.1006            0.12380   \n",
      "93            97.59       699.4            0.1339            0.17510   \n",
      "94           123.50      1025.0            0.1551            0.42030   \n",
      "95           156.10      1750.0            0.1190            0.35390   \n",
      "96            82.14       495.2            0.1140            0.09358   \n",
      "97            68.81       366.1            0.1316            0.09473   \n",
      "98            82.96       512.5            0.1431            0.18510   \n",
      "99           109.50       826.4            0.1431            0.30260   \n",
      "\n",
      "    concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0           0.71190               0.26540          0.4601   \n",
      "1           0.24160               0.18600          0.2750   \n",
      "2           0.45040               0.24300          0.3613   \n",
      "3           0.68690               0.25750          0.6638   \n",
      "4           0.40000               0.16250          0.2364   \n",
      "5           0.53550               0.17410          0.3985   \n",
      "6           0.37840               0.19320          0.3063   \n",
      "7           0.26780               0.15560          0.3196   \n",
      "8           0.53900               0.20600          0.4378   \n",
      "9           1.10500               0.22100          0.4366   \n",
      "10          0.14590               0.09975          0.2948   \n",
      "11          0.39650               0.18100          0.3792   \n",
      "12          0.36390               0.17670          0.3176   \n",
      "13          0.23220               0.11190          0.2809   \n",
      "14          0.69430               0.22080          0.3596   \n",
      "15          0.70260               0.17120          0.4218   \n",
      "16          0.29140               0.16090          0.3029   \n",
      "17          0.47840               0.20730          0.3706   \n",
      "18          0.53720               0.23880          0.2768   \n",
      "19          0.23900               0.12880          0.2977   \n",
      "20          0.18900               0.07283          0.3184   \n",
      "21          0.08867               0.06227          0.2450   \n",
      "22          0.63050               0.23930          0.4667   \n",
      "23          0.31550               0.20090          0.2822   \n",
      "24          0.46950               0.20950          0.3613   \n",
      "25          0.38530               0.25500          0.4066   \n",
      "26          0.55390               0.27010          0.4264   \n",
      "27          0.34460               0.14900          0.2341   \n",
      "28          0.63350               0.20240          0.4027   \n",
      "29          0.24890               0.14560          0.2756   \n",
      "..              ...                   ...             ...   \n",
      "70          0.26870               0.17890          0.2551   \n",
      "71          0.14340               0.04786          0.2254   \n",
      "72          0.65660               0.18990          0.3313   \n",
      "73          0.27790               0.13830          0.2589   \n",
      "74          0.17030               0.08660          0.2618   \n",
      "75          0.28290               0.15200          0.2650   \n",
      "76          0.08539               0.07407          0.2710   \n",
      "77          0.37860               0.21020          0.3751   \n",
      "78          0.76810               0.25080          0.5440   \n",
      "79          0.17310               0.07926          0.2779   \n",
      "80          0.17550               0.06127          0.2762   \n",
      "81          0.48580               0.17080          0.3527   \n",
      "82          0.64760               0.28670          0.2355   \n",
      "83          0.24320               0.18410          0.2311   \n",
      "84          0.22670               0.07632          0.3379   \n",
      "85          0.31570               0.16420          0.3695   \n",
      "86          0.33490               0.12250          0.3020   \n",
      "87          0.57550               0.19560          0.3956   \n",
      "88          0.24340               0.12050          0.2972   \n",
      "89          0.26040               0.13970          0.3151   \n",
      "90          0.09189               0.06946          0.2522   \n",
      "91          0.28460               0.14760          0.2556   \n",
      "92          0.13500               0.10010          0.2027   \n",
      "93          0.13810               0.07911          0.2678   \n",
      "94          0.52030               0.21150          0.2834   \n",
      "95          0.40980               0.15730          0.3689   \n",
      "96          0.04980               0.05882          0.2227   \n",
      "97          0.02049               0.02381          0.1934   \n",
      "98          0.19220               0.08449          0.2772   \n",
      "99          0.31940               0.15650          0.2718   \n",
      "\n",
      "    fractal_dimension_worst  \n",
      "0                   0.11890  \n",
      "1                   0.08902  \n",
      "2                   0.08758  \n",
      "3                   0.17300  \n",
      "4                   0.07678  \n",
      "5                   0.12440  \n",
      "6                   0.08368  \n",
      "7                   0.11510  \n",
      "8                   0.10720  \n",
      "9                   0.20750  \n",
      "10                  0.08452  \n",
      "11                  0.10480  \n",
      "12                  0.10230  \n",
      "13                  0.06287  \n",
      "14                  0.14310  \n",
      "15                  0.13410  \n",
      "16                  0.08216  \n",
      "17                  0.11420  \n",
      "18                  0.07615  \n",
      "19                  0.07259  \n",
      "20                  0.08183  \n",
      "21                  0.07773  \n",
      "22                  0.09946  \n",
      "23                  0.07526  \n",
      "24                  0.09564  \n",
      "25                  0.10590  \n",
      "26                  0.12750  \n",
      "27                  0.07421  \n",
      "28                  0.09876  \n",
      "29                  0.07919  \n",
      "..                      ...  \n",
      "70                  0.06589  \n",
      "71                  0.10840  \n",
      "72                  0.13390  \n",
      "73                  0.10300  \n",
      "74                  0.07609  \n",
      "75                  0.06387  \n",
      "76                  0.07191  \n",
      "77                  0.11080  \n",
      "78                  0.09964  \n",
      "79                  0.07918  \n",
      "80                  0.08851  \n",
      "81                  0.10160  \n",
      "82                  0.10510  \n",
      "83                  0.09203  \n",
      "84                  0.07924  \n",
      "85                  0.08579  \n",
      "86                  0.06846  \n",
      "87                  0.09288  \n",
      "88                  0.09261  \n",
      "89                  0.08473  \n",
      "90                  0.07246  \n",
      "91                  0.06828  \n",
      "92                  0.06206  \n",
      "93                  0.06603  \n",
      "94                  0.08234  \n",
      "95                  0.08368  \n",
      "96                  0.07376  \n",
      "97                  0.08988  \n",
      "98                  0.08756  \n",
      "99                  0.09353  \n",
      "\n",
      "[100 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "#using dummies to binarise the dataset\n",
    "#coverting the class lebel value to binary\n",
    "df['diagnosis']=pd.get_dummies(df['diagnosis'], prefix='diagnosis') \n",
    "#converting the class label into int\n",
    "#pd.to_numeric(df['diagnosis'], errors='coerce')\n",
    "\n",
    "print(df.head(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the dataset into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data was lost during the splitting process\n"
     ]
    }
   ],
   "source": [
    "mn = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[mn]\n",
    "df_test = df[~mn]\n",
    "#train.shape+test.shape\n",
    "#return true if no data is missin while spliting\n",
    " \n",
    "if (df_train.size+df_test.size==df.size):\n",
    "    print(\"no data was lost during the splitting process\")\n",
    "if (df_train.size+df_test.size!=df.size):\n",
    "    print(\"data was lost during the splitting process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the dataset as CSV after splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving into csv files without header or indexnumbers\n",
    "df.to_csv('breast_cancer_processed.csv',index=False,header=False)\n",
    "df_train.to_csv('breast_cancer_train.csv',index=False,header=False)\n",
    "df_test.to_csv('breast_cancer_test.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input, hidden and output nodes\n",
    "input_nodes = 30 \n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "\n",
    "learning_rate = 0.3\n",
    "batch_size = 50\n",
    "\n",
    "# epochs is the number of training iterations \n",
    "epochs = 10\n",
    "# datasets to read\n",
    "# you can change these when trying out other datasets\n",
    "train_file = \"breast_cancer_train.csv\"\n",
    "test_file = \"breast_cancer_test.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:  456\n",
      "test set size:  113\n",
      "0,17.99,10.38,122.8,1001.0,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05372999999999999,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019.0,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the mnist training data CSV file into a list\n",
    "#train_data_file = open(\"mnist/mnist_train_100.csv\", 'r') # open and read the 100 instances in the text file\n",
    "train_data_file = open(train_file, 'r')\n",
    "train_data_list = train_data_file.readlines() # read all lines into memory \n",
    "\n",
    "train_data_file.close() \n",
    "print(\"train set size: \", len(train_data_list))\n",
    "\n",
    "#testing the network\n",
    "#load the mnist test data CSV file into a list\n",
    "#test_data_file = open(\"mnist/mnist_test_10.csv\", 'r') # read the file with 10 instances first\n",
    "test_data_file = open(test_file, 'r') # read the file with 10 instances first\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "print(\"test set size: \", len(test_data_list))\n",
    "print(train_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    \"\"\"Artificial Neural Network classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    lr : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    ep : int\n",
    "      Number of epochs for training the network towards achieving convergence\n",
    "    batch_size : int\n",
    "      Size of the training batch to be used when calculating the gradient descent. \n",
    "      batch_size = 0 standard gradient descent\n",
    "      batch_size > 0 stochastic gradient descent \n",
    "\n",
    "    inodes : int\n",
    "      Number of input nodes which is normally the number of features in an instance.\n",
    "    hnodes : int\n",
    "      Number of hidden nodes in the net.\n",
    "    onodes : int\n",
    "      Number of output nodes in the net.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    wih : 2d-array\n",
    "      Input2Hidden node weights after fitting \n",
    "    who : 2d-array\n",
    "      Hidden2Output node weights after fitting \n",
    "    E : list\n",
    "      Sum-of-squares error value in each epoch.\n",
    "      \n",
    "    Results : list\n",
    "      Target and predicted class labels for the test data.\n",
    "      \n",
    "    Functions\n",
    "    ---------\n",
    "    activation_function : float (between 1 and -1)\n",
    "        implments the sigmoid function which squashes the node input\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputnodes=30, hiddennodes=200, outputnodes=10, learningrate=0.1, batch_size=50, epochs=10):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        #link weight matrices, wih (input to hidden) and who (hidden to output)\n",
    "        #a weight on link from node i to node j is w_ij\n",
    "        \n",
    "        \n",
    "        #Draw random samples from a normal (Gaussian) distribution centered around 0.\n",
    "        #numpy.random.normal(loc to centre gaussian=0.0, scale=1, size=dimensions of the array we want) \n",
    "        #scale is usually set to the standard deviation which is related to the number of incoming links i.e. \n",
    "        #1/sqrt(num of incoming inputs). we use pow to raise it to the power of -0.5.\n",
    "        #We have set 0 as the centre of the guassian dist.\n",
    "        # size is set to the dimensions of the number of hnodes, inodes and onodes\n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        #set the learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        #set the batch size\n",
    "        self.bs = batch_size\n",
    "        \n",
    "        #set the number of epochs\n",
    "        self.ep = epochs\n",
    "        \n",
    "        #store errors at each epoch\n",
    "        self.E= []\n",
    "        \n",
    "        #store results from testing the model\n",
    "        #keep track of the network performance on each test instance\n",
    "        self.results= []\n",
    "        \n",
    "        #define the activation function here\n",
    "        #specify the sigmoid squashing function. Here expit() provides the sigmoid function.\n",
    "        #lambda is a short cut function which is executed there and then with no def (i.e. like an anonymous function)\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "   \n",
    "    def batch_input(self, input_list):\n",
    "        \"\"\"Yield consecutive batches of the specified size from the input list.\"\"\"\n",
    "        for i in range(0, len(input_list), self.bs):\n",
    "            yield input_list[i:i + self.bs]\n",
    "    \n",
    "    #train the neural net\n",
    "    #note the first part is very similar to the query function because they both require the forward pass\n",
    "    def train(self, train_inputs):\n",
    "        \"\"\"Training the neural net. \n",
    "           This includes the forward pass ; error computation; \n",
    "           backprop of the error ; calculation of gradients and updating the weights.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_inputs : {array-like}, shape = [n_instances, n_features]\n",
    "            Training vectors, where n_instances is the number of training instances and\n",
    "            n_features is the number of features.\n",
    "            Note this contains all features including the class feature which is in first position\n",
    "        \n",
    "            Returns\n",
    "            -------\n",
    "            self : object\n",
    "        \"\"\"\n",
    "      \n",
    "        for e in range(self.ep):\n",
    "            print(\"Training epoch#: \", e)\n",
    "            sum_error = 0.0   \n",
    "            for batch in self.batch_input(train_inputs):\n",
    "                #creating variables to store the gradients   \n",
    "                delta_who = 0\n",
    "                delta_wih = 0\n",
    "                \n",
    "                # iterate through the inputs sent in\n",
    "                for instance in batch:\n",
    "                    # split it by the commas\n",
    "                    all_values = instance.split(',') \n",
    "                    # scale and shift the inputs to address the problem of diminishing weights due to multiplying by zero\n",
    "                    # divide the raw inputs which are in the range 0-255 by 255 will bring them into the range 0-1\n",
    "                    # multiply by 0.99 to bring them into the range 0.0 - 0.99.\n",
    "                    # add 0.01 to shift them up to the desired range 0.01 - 1. \n",
    "                    inputs = (np.asfarray(all_values[1:]))+ 0.01 #/ 255.0 * 0.99) + 0.01\n",
    "                    #create the target output values for each instance so that we can use it with the neural net\n",
    "                    #note we need 10 nodes where each represents one of the digits\n",
    "                    targets = np.zeros(output_nodes)+ 0.01 #all initialised to 0.01\n",
    "                    #all_value[0] has the target class label for this instance\n",
    "                    targets[int(all_values[0])] = 0.99\n",
    "        \n",
    "                    #convert  inputs list to 2d array\n",
    "                    inputs = np.array(inputs,  ndmin=2).T\n",
    "                    targets = np.array(targets, ndmin=2).T\n",
    "\n",
    "                    #calculate signals into hidden layer\n",
    "                    hidden_inputs = np.dot(self.wih, inputs)\n",
    "                    #calculate the signals emerging from the hidden layer\n",
    "                    hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "                    #calculate signals into final output layer\n",
    "                    final_inputs=np.dot(self.who, hidden_outputs)\n",
    "                    #calculate the signals emerging from final output layer\n",
    "                    final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "                    #to calculate the error we need to compute the element wise diff between target and actual\n",
    "                    output_errors = targets - final_outputs\n",
    "                    #Next distribute the error to the hidden layer such that hidden layer error\n",
    "                    #is the output_errors, split by weights, recombined at hidden nodes\n",
    "                    hidden_errors = np.dot(self.who.T, output_errors)\n",
    "            \n",
    "                       \n",
    "                    ## for each instance accumilate the gradients from each instance\n",
    "                    ## delta_who are the gradients between hidden and output weights\n",
    "                    ## delta_wih are the gradients between input and hidden weights\n",
    "                    delta_who += np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "                    delta_wih += np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "                    \n",
    "                    sum_error += np.dot(output_errors.T, output_errors)#this is the sum of squared error accumilated over each batced instance\n",
    "                   \n",
    "                pass #instance\n",
    "            \n",
    "                # update the weights by multiplying the gradient with the learning rate\n",
    "                # note that the deltas are divided by batch size to obtain the average gradient according to the given batch\n",
    "                # obviously if batch size = 1 then we dont need to bother with an average\n",
    "                self.who += self.lr * (delta_who / self.bs)\n",
    "                self.wih += self.lr * (delta_wih / self.bs)\n",
    "            pass # batch\n",
    "            self.E.append(np.asfarray(sum_error).flatten())\n",
    "            print(\"errors (SSE): \", self.E[-1])\n",
    "        pass # epoch\n",
    "    \n",
    "    #query the neural net\n",
    "    def query(self, inputs_list):\n",
    "        #convert inputs_list to a 2d array\n",
    "        #print(numpy.matrix(inputs_list))\n",
    "        #inputs_list [[ 1.   0.5 -1.5]]\n",
    "        inputs = np.array(inputs_list, ndmin=2).T \n",
    "        #once converted it appears as follows\n",
    "        #[[ 1. ]\n",
    "        # [ 0.5]\n",
    "        # [-1.5]]\n",
    "        #print(numpy.matrix(inputs))\n",
    "        \n",
    "        #propogate input into hidden layer. This is the start of the forward pass\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        \n",
    "        #squash the content in the hidden node using the sigmoid function (value between 1, -1)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "                \n",
    "        #propagate into output layer and the apply the squashing sigmoid function\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n",
    "    \n",
    "     \n",
    "    #iterate through all the test data to calculate model accuracy\n",
    "    def test(self, test_inputs):\n",
    "        self.results = []\n",
    "        \n",
    "        #go through each test instances\n",
    "        for instance in test_inputs:\n",
    "            all_values = instance.split(',') # extract the input feature values for the instance\n",
    "    \n",
    "            target_label = int(all_values[0]) # get the target class for the instance\n",
    "    \n",
    "            #scale and shift the inputs this is to make sure values dont lead to zero when multiplied with weights\n",
    "            inputs = (np.asfarray(all_values[1:]))+ 0.01 # / 255.0 * 0.99) + 0.01\n",
    "    \n",
    "            #query the network with test inputs\n",
    "            #note this returns 10 output values ; of which the index of the highest value\n",
    "            # is the networks predicted class label\n",
    "            outputs = self.query(inputs)\n",
    "    \n",
    "            #get the index of the highest output node as this corresponds to the predicted class\n",
    "            predict_label = np.argmax(outputs) #this is the class predicted by the ANN\n",
    "    \n",
    "            self.results.append([predict_label, target_label])\n",
    "            #compute network error\n",
    "            #if (predict_label == target_label):\n",
    "            #    self.results.append(1)\n",
    "            #else: \n",
    "            #    self.results.append(0)\n",
    "            pass\n",
    "        pass\n",
    "        self.results = np.asfarray(self.results) # flatten results to avoid nested arrays\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch#:  0\n",
      "errors (SSE):  [655.18474673]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [244.5786846]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [230.26539015]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [237.95276458]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [237.91117561]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [237.88112038]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [237.85869141]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [237.84124191]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [237.82722141]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [237.81566529]\n"
     ]
    }
   ],
   "source": [
    "#create instance of neuralnet\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate, batch_size, epochs)\n",
    "\n",
    "# numpy.random.choice generates a random sample from a given 1-D array\n",
    "# we can use this to select a sample from our training data in case we want to work with a small sample\n",
    "# for instance we use a small sample here such as 1500\n",
    "#mini_training_data = np.random.choice(train_data_list,400,replace = False)\n",
    "#print(\"Percentage of training data used:\", (len(mini_training_data)/len(train_data_list)) * 100)\n",
    "n.train(train_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the model error and epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print the error over epochs\n",
    "plt.figure(figsize=(20,5)) #width, height settings for figures\n",
    "plt.plot(range(0, n.ep), np.asfarray(n.E), marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of updates')\n",
    "plt.title('ANN with Various epoches(Breast Cancer Dataset)')\n",
    "# plt.savefig('images/02_07.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the ANN and compute the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy% =  63.716814159292035\n"
     ]
    }
   ],
   "source": [
    "n.test(test_data_list)\n",
    "#print network performance as an accuracy metric\n",
    "correct = 0 # number of predictions that were correct\n",
    "input_nodes = 30\n",
    "\n",
    "#iteratre through each tested instance and accumilate number of correct predictions\n",
    "for result in n.results:\n",
    "    if (result[0] == result[1]):\n",
    "            correct += 1\n",
    "    pass\n",
    "pass\n",
    "\n",
    "# print the accuracy on test set\n",
    "print (\"Test set accuracy% = \", (100 * correct / len(n.results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN with various epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take a few moments ...\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [979.59878422]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [319.43416979]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [679.58087143]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [274.86374528]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [243.79344376]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [221.04715463]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [217.28169354]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [904.59916954]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [269.86598684]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [232.33975424]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [218.80270135]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [216.45289584]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [215.81925344]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [216.3407775]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [215.99625753]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [215.73849638]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [215.53527369]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [822.26468417]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [397.92137964]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [214.2196219]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [216.85786496]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [220.75745912]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [216.32013488]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [215.9099369]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [215.64238677]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [215.45221486]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [215.30916203]\n",
      "Training epoch#:  10\n",
      "errors (SSE):  [215.19733693]\n",
      "Training epoch#:  11\n",
      "errors (SSE):  [215.10743557]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [457.59857761]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [232.74862611]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [223.15990581]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [220.18677274]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [216.53780727]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [215.48042357]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [216.31283038]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [215.70233401]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [215.39574331]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [215.19065424]\n",
      "Training epoch#:  10\n",
      "errors (SSE):  [215.04280173]\n",
      "Training epoch#:  11\n",
      "errors (SSE):  [214.93085613]\n",
      "Training epoch#:  12\n",
      "errors (SSE):  [214.84306183]\n",
      "Training epoch#:  13\n",
      "errors (SSE):  [214.77232507]\n",
      "Training epoch#:  14\n",
      "errors (SSE):  [214.71404035]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [480.66233416]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [222.65615947]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [221.36185142]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [217.92496896]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [215.64293976]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [215.09665966]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [214.78778979]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [214.58174628]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [214.43257093]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [214.31860102]\n",
      "Training epoch#:  10\n",
      "errors (SSE):  [214.22780961]\n",
      "Training epoch#:  11\n",
      "errors (SSE):  [214.15164883]\n",
      "Training epoch#:  12\n",
      "errors (SSE):  [214.07879085]\n",
      "Training epoch#:  13\n",
      "errors (SSE):  [214.01066823]\n",
      "Training epoch#:  14\n",
      "errors (SSE):  [213.9480184]\n",
      "Training epoch#:  15\n",
      "errors (SSE):  [213.51820744]\n",
      "Training epoch#:  16\n",
      "errors (SSE):  [214.52484059]\n",
      "Training epoch#:  17\n",
      "errors (SSE):  [213.89547059]\n",
      "Training epoch#:  18\n",
      "errors (SSE):  [213.87152868]\n",
      "Training epoch#:  19\n",
      "errors (SSE):  [213.84933105]\n"
     ]
    }
   ],
   "source": [
    "print(\"This will take a few moments ...\")\n",
    "n_list = []\n",
    "epochs = [2,5,10,12,15,20]\n",
    "\n",
    "for epoch in epochs:\n",
    "    n = neuralNetwork(epochs=epoch)\n",
    "    n.train(train_data_list)\n",
    "    n_list.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather the results from various size of epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.716814159292035\n",
      "63.716814159292035\n",
      "63.716814159292035\n",
      "63.716814159292035\n",
      "63.716814159292035\n",
      "63.716814159292035\n"
     ]
    }
   ],
   "source": [
    "#iteratre through each model and accumilate number of correct predictions\n",
    "model_results = []\n",
    "for model in n_list: \n",
    "    correct = 0\n",
    "    model.test(test_data_list)\n",
    "    for result in model.results:\n",
    "        if (result[0] == result[1]):\n",
    "                correct += 1\n",
    "        pass\n",
    "    correct = 100 * (correct/len(model.results))\n",
    "    model_results.append(correct)\n",
    "    print(correct)\n",
    "    pass\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results from various size of epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects =epochs \n",
    "y_pos = np.arange(len(objects))\n",
    "performance = model_results\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoches')\n",
    "plt.title('ANN with Various Size of epoches(Breast Cancer Dataset)')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN with various Batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take a few moments ...\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [222.29294026]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [217.44283183]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [217.3506399]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [218.04423759]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [217.71962845]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [217.71234324]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [217.70836243]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [217.70591764]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [217.70426503]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [217.70314875]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [1093.54302249]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [367.15481092]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [232.245112]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [222.13177536]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [220.51679]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [219.04007708]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [219.20123329]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [218.66298019]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [217.02852279]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [216.59662915]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [1021.45803234]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [293.37819015]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [253.38738903]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [235.88712109]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [227.91695611]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [224.2775524]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [222.27850486]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [221.01948491]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [220.0593765]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [219.33234952]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [1679.32076509]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [957.8686809]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [642.58153639]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [465.47900488]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [431.05821846]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [405.30077204]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [356.69523084]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [288.94744265]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [244.55066669]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [230.31363792]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [2199.26739624]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [1612.80593077]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [1306.44436011]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [1016.28314288]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [677.24197962]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [540.31046921]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [359.90600458]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [251.71418027]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [235.14188711]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [228.04666147]\n"
     ]
    }
   ],
   "source": [
    "# numpy.random.choice generates a random sample from a given 1-D array\n",
    "# we can use this to select a sample from our training data in case we want to work with a small sample\n",
    "# for instance we use a small sample here such as 1500\n",
    "\n",
    "#mini_training_data = np.random.choice(train_data_list,300, replace = False)\n",
    "#print(\"Percentage of training data used:\", (len(mini_training_data)/len(train_data_list)) * 100)\n",
    "\n",
    "print(\"This will take a few moments ...\")\n",
    "n_list = []\n",
    "batch_sizes = [1,100,250,350,450]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    n = neuralNetwork(batch_size=batch_size)\n",
    "    n.train(train_data_list)\n",
    "    n_list.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather the results from various batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.716814159292035\n",
      "63.716814159292035\n",
      "63.716814159292035\n",
      "63.716814159292035\n",
      "63.716814159292035\n"
     ]
    }
   ],
   "source": [
    "#iteratre through each model and accumilate number of correct predictions\n",
    "model_results = []\n",
    "for model in n_list: \n",
    "    correct = 0\n",
    "    model.test(test_data_list)\n",
    "    for result in model.results:\n",
    "        if (result[0] == result[1]):\n",
    "                correct += 1\n",
    "        pass\n",
    "    correct = 100 * (correct/len(model.results))\n",
    "    model_results.append(correct)\n",
    "    print(correct)\n",
    "    pass\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results from various batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects = batch_sizes\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = model_results\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('batch_size')\n",
    "plt.title('ANN with Various Batch Size(Breast Cancer Dataset)')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN with various learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take a few moments ...\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [507.60280525]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [223.71918452]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [219.39936979]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [217.72552865]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [216.84204338]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [216.29459391]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [215.92215463]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [215.65208498]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [215.4428702]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [214.75852139]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [570.13340095]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [224.3121766]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [223.18024501]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [220.00244953]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [220.16822911]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [219.94517258]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [219.8001611]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [219.69780956]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [219.62160413]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [219.56264399]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [362.39357298]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [247.11890477]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [239.52799713]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [239.08514512]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [238.86064871]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [238.78402941]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [238.73246853]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [238.66165129]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [238.61790972]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [238.58430962]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [493.7138945]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [273.54155981]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [272.37695547]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [276.38706365]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [276.20640813]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [276.13561775]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [276.09742355]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [276.07109346]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [276.05002368]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [276.03095969]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [514.6979741]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [298.57040206]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [341.02240422]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [313.88134914]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [304.0993812]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [345.9807052]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [293.8427526]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [327.53160604]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [317.5791426]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [332.56296026]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [336.9580977]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [312.3096581]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [423.97919629]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [312.47805042]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [458.08441942]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [404.69466463]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [452.9260107]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [445.73995025]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [344.6444483]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [450.33848554]\n",
      "Training epoch#:  0\n",
      "errors (SSE):  [552.39781447]\n",
      "Training epoch#:  1\n",
      "errors (SSE):  [448.8665908]\n",
      "Training epoch#:  2\n",
      "errors (SSE):  [565.87707155]\n",
      "Training epoch#:  3\n",
      "errors (SSE):  [558.58712175]\n",
      "Training epoch#:  4\n",
      "errors (SSE):  [558.31460001]\n",
      "Training epoch#:  5\n",
      "errors (SSE):  [557.3988031]\n",
      "Training epoch#:  6\n",
      "errors (SSE):  [451.95399854]\n",
      "Training epoch#:  7\n",
      "errors (SSE):  [447.10719397]\n",
      "Training epoch#:  8\n",
      "errors (SSE):  [447.08818319]\n",
      "Training epoch#:  9\n",
      "errors (SSE):  [447.03393976]\n"
     ]
    }
   ],
   "source": [
    "# numpy.random.choice generates a random sample from a given 1-D array\n",
    "# we can use this to select a sample from our training data in case we want to work with a small sample\n",
    "# for instance we use a small sample here such as 1500\n",
    "#mini_training_data = np.random.choice(train_data_list, 300, replace = False)\n",
    "#print(\"Percentage of training data used:\", (len(mini_training_data)/len(train_data_list)) * 100)\n",
    "\n",
    "print(\"This will take a few moments ...\")\n",
    "n_list = []\n",
    "learning_rates = [0.1,0.2,0.3,0.4,0.6,0.8,1.0]\n",
    "\n",
    "for learn in learning_rates:\n",
    "    n = neuralNetwork(learningrate = learn,batch_size=50)\n",
    "    n.train(train_data_list)\n",
    "    n_list.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gathering the results from various learing rate ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.716814159292035\n",
      "63.716814159292035\n",
      "63.716814159292035\n",
      "63.716814159292035\n",
      "63.716814159292035\n",
      "36.283185840707965\n",
      "36.283185840707965\n"
     ]
    }
   ],
   "source": [
    "#iteratre through each model and accumilate number of correct predictions\n",
    "model_results = []\n",
    "for model in n_list: \n",
    "    correct = 0\n",
    "    model.test(test_data_list)\n",
    "    for result in model.results:\n",
    "        if (result[0] == result[1]):\n",
    "                correct += 1\n",
    "        pass\n",
    "    correct = 100 * (correct/len(model.results))\n",
    "    model_results.append(correct)\n",
    "    print(correct)\n",
    "    pass\n",
    "pass\n",
    "\n",
    "\n",
    "# print the accuracy on test set\n",
    "#print (\"Test set accuracy% = \", (100 * correct / len(n.results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the accuray from Various learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects = learning_rates\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = model_results\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning_rate')\n",
    "plt.title('ANN with Various Learning rate(Breast Cancer Dataset)')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
